import tempfile

import streamlit as st
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI

from document_processor import DocumentProcessor
from loaders import carrega_pdf
pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"
CONFIG_MODELOS = {'Groq': 
                        {'modelos': ['llama-3.1-70b-versatile', 'gemma2-9b-it', 'mixtral-8x7b-32768'],
                         'chat': ChatGroq},
                'OpenAI': 
                        {'modelos': ['gpt-4o-mini', 'gpt-4o', 'o1-preview', 'o1-mini'],
                         'chat': ChatOpenAI}}

# Inicializa mem√≥ria global
MEMORIA = ConversationBufferMemory()

def carrega_arquivo(arquivo):
    """Carrega arquivo PDF."""
    with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp:
        temp.write(arquivo.read())
        nome_temp = temp.name
    return carrega_pdf(nome_temp)

def carrega_modelo(provedor, modelo, api_key, arquivo):
    """Carrega o modelo de linguagem e processa o documento."""
    if not arquivo:
        st.error('Por favor, fa√ßa upload de um arquivo')
        return
    
    # Container para progresso
    progress_container = st.sidebar.empty()
    with progress_container.container():
        st.markdown("#### Processando documento...")
        progress = st.progress(0)
        
        # Carrega arquivo
        progress.progress(25, "Carregando PDF...")
        documento = carrega_arquivo(arquivo)
        
        # Processa documento
        progress.progress(50, "Processando texto...")
        chat = CONFIG_MODELOS[provedor]['chat'](model=modelo, api_key=api_key)
        processor = DocumentProcessor(chat)
        chunks = processor.process_document(documento)
        
        # Configura modelo
        progress.progress(75, "Configurando modelo...")
        st.session_state['chunks'] = chunks
        st.session_state['processor'] = processor
        st.session_state['memoria'] = MEMORIA
        
        # Define template
        template = ChatPromptTemplate.from_messages([
            ('system', '''Voc√™ √© um assistente amig√°vel chamado Or√°culo.
            
            {context}
            
            Utilize as informa√ß√µes fornecidas para basear as suas respostas.
            Se a informa√ß√£o n√£o estiver nas se√ß√µes fornecidas, indique que precisa 
            consultar outras partes do documento.
            
            HIST√ìRICO DA CONVERSA:
            {chat_history}'''),
            ('user', '{input}')
        ])
        
        chain = template | chat
        st.session_state['chain'] = chain
        
        # Finaliza
        progress.progress(100, "Pronto!")
        st.success("Documento processado com sucesso!")
    
    # Remove container de progresso ap√≥s 2 segundos
    import time
    time.sleep(2)
    progress_container.empty()

def pagina_chat():
    st.header('ü§ñBem-vindo ao Or√°culo', divider=True)

    chain = st.session_state.get('chain')
    if chain is None:
        st.error('Carrege o Or√°culo')
        st.stop()

    # Recupera mem√≥ria da sess√£o
    memoria = st.session_state.get('memoria', MEMORIA)
    
    # Mostra hist√≥rico
    for mensagem in memoria.buffer_as_messages:
        chat = st.chat_message(mensagem.type)
        chat.markdown(mensagem.content)

    input_usuario = st.chat_input('Fale com o or√°culo')
    if input_usuario:
        chat = st.chat_message('human')
        chat.markdown(input_usuario)

        chat = st.chat_message('ai')
        processor = st.session_state.get('processor')
        chunks = st.session_state.get('chunks')
        
        # Pega contexto relevante baseado na pergunta
        context = processor.get_context(input_usuario, chunks)
        
        resposta = chat.write_stream(chain.stream({
            'input': input_usuario,
            'context': context,
            'chat_history': memoria.buffer_as_messages
        }))
        
        # Atualiza mem√≥ria
        memoria.chat_memory.add_user_message(input_usuario)
        memoria.chat_memory.add_ai_message(resposta)
        st.session_state['memoria'] = memoria

def sidebar():
    with st.sidebar:
        # Informa√ß√µes do autor
        st.markdown("### ‚ö° Zeus - Assistente Jur√≠dico")
        st.markdown("""
        *Desenvolvido por:*  
        **Jo√£o Val√©rio**  
        Juiz do Tribunal de Justi√ßa do Par√°  
        joao.moura@tjpa.jus.br
        """)
        
        st.divider()
        
        # Upload e configura√ß√µes
        arquivo = st.file_uploader('Fa√ßa upload do arquivo PDF', type=['.pdf'])
        provedor = st.selectbox('Selecione o provedor dos modelo', CONFIG_MODELOS.keys())
        modelo = st.selectbox('Selecione o modelo', CONFIG_MODELOS[provedor]['modelos'])
        api_key = st.text_input(
            f'Adicione a api key para o provedor {provedor}',
            value=st.session_state.get(f'api_key_{provedor}'))
        st.session_state[f'api_key_{provedor}'] = api_key
    
        if st.button('Inicializar Zeus', use_container_width=True):
            carrega_modelo(provedor, modelo, api_key, arquivo)
            
        if st.button('Apagar Hist√≥rico', use_container_width=True):
            st.session_state['memoria'] = MEMORIA

def main():
    with st.sidebar:
        sidebar()
    pagina_chat()

if __name__ == '__main__':
    main()
